{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle房价预测.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuanyibo666/DL_practicecode/blob/main/kaggle%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **下载和缓存数据集**\n",
        "+ 建立字典DATA_HUB。\n",
        "+ download函数用来下载数据集， 将数据集缓存在本地目录，并且其sha-1与存储在DATA_HUB中的相匹配。\n",
        "+ download_extract函数下载并解压缩一个zip或tar文件。\n",
        "+ download_all函数将本书中使用的所有数据集从DATA_HUB下载到缓存目录中"
      ],
      "metadata": {
        "id": "Em0jonyL4ws-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlKYgY5czYzF"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import os\n",
        "import tarfile\n",
        "import zipfile\n",
        "import requests\n",
        "import random\n",
        "\n",
        "DATA_HUB = dict()\n",
        "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
        "\n",
        "def download(name, cache_dir=os.path.join('..', 'data')):  \n",
        "    \"\"\"下载一个DATA_HUB中的文件，返回本地文件名\"\"\"\n",
        "    assert name in DATA_HUB, f\"{name} 不存在于 {DATA_HUB}\"\n",
        "    url, sha1_hash = DATA_HUB[name]\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname  # 命中缓存\n",
        "    print(f'正在从{url}下载{fname}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname\n",
        "\n",
        "def download_extract(name, folder=None):  \n",
        "    \"\"\"下载并解压zip/tar文件\"\"\"\n",
        "    fname = download(name)\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    if ext == '.zip':\n",
        "        fp = zipfile.ZipFile(fname, 'r')\n",
        "    elif ext in ('.tar', '.gz'):\n",
        "        fp = tarfile.open(fname, 'r')\n",
        "    else:\n",
        "        assert False, '只有zip/tar文件可以被解压缩'\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "def download_all():  \n",
        "    \"\"\"下载DATA_HUB中的所有文件\"\"\"\n",
        "    for name in DATA_HUB:\n",
        "        download(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **访问并读取数据集**\n",
        "  使用pandas读入并处理数据，并分别加载包含训练数据和测试数据的两个CSV文件。\n",
        " \n",
        "  训练数据集包括1460个样本，每个样本80个特征和1个标签， 而测试数据集包含1459个样本，每个样本80个特征。\n"
      ],
      "metadata": {
        "id": "mZD2w5D05ECH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pvbAmh7x0F2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_HUB['kaggle_house_train'] = (  \n",
        "    DATA_URL + 'kaggle_house_pred_train.csv',\n",
        "    '585e9cc93e70b39160e7921475f9bcd7d31219ce')\n",
        "\n",
        "DATA_HUB['kaggle_house_test'] = (  \n",
        "    DATA_URL + 'kaggle_house_pred_test.csv',\n",
        "    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')\n",
        "train_data = pd.read_csv(download('kaggle_house_train'))\n",
        "test_data = pd.read_csv(download('kaggle_house_test'))\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jr8y8Dr0I_C",
        "outputId": "3d9becbc-d70b-4a7c-9f93-3f6eeff73805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1460, 81)\n",
            "(1459, 80)\n",
            "   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n",
            "0   1          60       RL         65.0       WD        Normal     208500\n",
            "1   2          20       RL         80.0       WD        Normal     181500\n",
            "2   3          60       RL         68.0       WD        Normal     223500\n",
            "3   4          70       RL         60.0       WD       Abnorml     140000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))"
      ],
      "metadata": {
        "id": "9hZBNdWm0LXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **数据预处理**\n",
        "+ 将所有缺失的值替换为相应特征的平均值\n",
        "+ 为了将所有特征放在一个共同的尺度上，通过将特征重新缩放到零均值和单位方差来标准化数据：\n",
        "$$x\\leftarrow\\frac{x-\\mu}{\\sigma}$$\n",
        "$\\mu$表示均值，$\\sigma$表示标准差，经标准化后，这些特征具有零均值和单位方差。\n",
        "+ 用one-hot编码处理离散值。\n",
        "\n",
        "  处理后，特征的总数量从79个增加到331个，通过value属性，从pandas格式中提取NumPy格式，并将其转换为张量表示用于训练。"
      ],
      "metadata": {
        "id": "MtaFStqo-WOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 若无法获得测试数据，则可根据训练数据计算均值和标准差\n",
        "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
        "all_features[numeric_features] = all_features[numeric_features].apply(\n",
        "    lambda x: (x - x.mean()) / (x.std()))\n",
        "# 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0\n",
        "all_features[numeric_features] = all_features[numeric_features].fillna(0)\n",
        "# “Dummy_na=True”将“na”（缺失值）视为有效的特征值，并为其创建指示符特征\n",
        "all_features = pd.get_dummies(all_features, dummy_na=True)\n",
        "all_features.shape\n",
        "n_train = train_data.shape[0]\n",
        "train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)\n",
        "test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\n",
        "train_labels = torch.tensor(\n",
        "    train_data.SalePrice.values.reshape(-1, 1), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "vb6kCFmw0NZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_features)\n",
        "print(train_features.shape)\n",
        "print(test_features.shape)\n",
        "print(train_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqrEzAB62f9H",
        "outputId": "8eb6350e-7bad-41b6-8a51-8c49fde94642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0673, -0.1844, -0.2178,  ...,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.8735,  0.4581, -0.0720,  ...,  1.0000,  0.0000,  0.0000],\n",
            "        [ 0.0673, -0.0559,  0.1372,  ...,  1.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.3025, -0.1416, -0.1428,  ...,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.8735, -0.0559, -0.0572,  ...,  1.0000,  0.0000,  0.0000],\n",
            "        [-0.8735,  0.2439, -0.0293,  ...,  1.0000,  0.0000,  0.0000]])\n",
            "torch.Size([1460, 331])\n",
            "torch.Size([1459, 331])\n",
            "torch.Size([1460, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **生成迭代器**\n",
        "\n",
        "  为了在训练模型时对数据集进行遍历，每次抽取一小批量样本并使用它们来更新模型。\n",
        "\n",
        "  定义一个**data_iter**函数，该函数接受**批量大小、特征矩阵和标签向量**作为输入，每次生成大小为**batch_size**的小批量，每个小批量包含一组特征和标签。每次选取小批量时随机选取。"
      ],
      "metadata": {
        "id": "OygCj0nKwXP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_iter(batch_size, features, labels):\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    # 这些样本是随机读取的，没有特定的顺序\n",
        "    random.shuffle(indices)\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "        batch_indices = torch.tensor(\n",
        "            indices[i: min(i + batch_size, num_examples)])\n",
        "        yield features[batch_indices], labels[batch_indices]\n",
        "        # yield返回生成器，下次调用data_iter时从上次yield位置继续执行"
      ],
      "metadata": {
        "id": "XPHF3yUI0TWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **定义激活函数ReLU**\n",
        "ReLU函数其实是分段线性函数，把所有的负值都变为0，而正值不变，这种操作被成为单侧抑制。\n",
        "\n",
        "ReLU函数的数学原理如下：\n",
        "$$RELU(x)=\\max(x,0)$$\n",
        "\n",
        "#### 激活函数评估：\n",
        "+ 优点：ReLu具有稀疏性，可以使稀疏后的模型能够更好地挖掘相关特征，拟合训练数据；在x>0区域上，不会出现梯度饱和、梯度消失的问题；计算复杂度低，不需要进行指数运算，只要一个阈值就可以得到激活值。\n",
        "+ 不足：输出不是0对称；由于小于0的时候激活函数值为0，梯度为0，所以存在一部分神经元永远不会得到更新。"
      ],
      "metadata": {
        "id": "L27fYk8KwsRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#激活函数\n",
        "def relu(X):\n",
        "    a = torch.zeros_like(X)\n",
        "    return torch.max(X, a)"
      ],
      "metadata": {
        "id": "fE8frrtl0WBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **将线性网络的结果进行映射**\n",
        "\n",
        "$$\\hat{y_i}=e^{y_i}$$\n",
        "+ 目的：使$\\hat{y}$大于零,便于损失函数的计算。"
      ],
      "metadata": {
        "id": "dAfKQY6gxQ5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_exp(X):\n",
        "  X_exp=torch.exp(X)\n",
        "  return X_exp"
      ],
      "metadata": {
        "id": "RszdDaROtxzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **定义MLP网络**\n",
        "+ 输入层：331个节点\n",
        "+ 隐藏层（1层）：256个节点\n",
        "+ 输出层：1个节点"
      ],
      "metadata": {
        "id": "IcbqiO0lyLJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#计算模型\n",
        "def net(X,W1,W2,W3,b1,b2,b3):\n",
        "    H1 = relu(torch.matmul(X,W1) + b1)\n",
        "    H2 = relu(torch.matmul(H1,W2)+b2)\n",
        "    return my_exp(torch.matmul(H2,W3) + b3)"
      ],
      "metadata": {
        "id": "jbfepskp0YIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **定义损失函数**\n",
        "房价就像股票价格一样，关心的是相对数量，而不是绝对数量。因此，**更关心相对误差$\\frac{y - \\hat{y}}{y}$，**\n",
        "而不是绝对误差$y - \\hat{y}$。\n",
        "\n",
        "即将$\\delta$ for $|\\log y - \\log \\hat{y}| \\leq \\delta$\n",
        "转换为$e^{-\\delta} \\leq \\frac{\\hat{y}}{y} \\leq e^\\delta$。\n",
        "这使得预测价格的对数与真实标签价格的对数之间出现以下均方根误差：\n",
        "$$\\sqrt{\\frac{1}{n}\\sum^n_{i=1}(\\log y_i-\\log \\hat{y}_i)^2}$$\n",
        "对此，我们自定义了**CrossEntropyLoss函数**，该函数以$\\hat{y}$和$y$为输入，实现以上损失计算。"
      ],
      "metadata": {
        "id": "t5XGnaEOyyEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CrossEntropyLoss(y_hat,y):\n",
        "    X=(torch.log(y)-torch.log(y_hat))**2\n",
        "    return (X.mean())**0.5"
      ],
      "metadata": {
        "id": "2FoBFGkXX_qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **定义优化函数**\n",
        "为了实现小批量随机梯度下降，我们定义了**sgd函数**，该函数以模型参**数集合、学习速率和批量大小**为输入。每一步更新的大小由学习速率**lr**决定，计算的损失grad是一个批量样本损失的总和，所以用批量大小batch_size来规范化损失，即**grad/batch_size**，这样损失大小就不会取决于我们对批量大小的选择。"
      ],
      "metadata": {
        "id": "XuA0kvsj0ZAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(params, lr, batch_size):\n",
        "    \"\"\"小批量随机梯度下降\"\"\"\n",
        "    with torch.no_grad():\n",
        "      for param in params:\n",
        "        param.data -=lr*param.grad/batch_size\n",
        "        param.grad.zero_()"
      ],
      "metadata": {
        "id": "UN9siIGbYB74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **定义超参数**"
      ],
      "metadata": {
        "id": "3MMVT0UN34Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 150\n",
        "lr = 0.5\n",
        "batch_size=10"
      ],
      "metadata": {
        "id": "-Lld5IKP39ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **初始化模型参数**\n",
        "通过从均值为0，标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0。\n",
        "\n",
        "其中W1、b1分别为输入层到隐藏层的权重和偏置，W2、b2分别为隐藏层到输出层的权重和偏置。"
      ],
      "metadata": {
        "id": "L6kCiQ3z4FeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#初始化模型参数,设置256个节点\n",
        "W1 = torch.normal(0, 0.01, size=(331,256), requires_grad=True)\n",
        "W2 = torch.normal(0, 0.01, size=(256,128),requires_grad=True)\n",
        "W3 = torch.normal(0, 0.01, size=(128,1),requires_grad=True)\n",
        "b1 = torch.zeros(256,requires_grad=True)\n",
        "b2 = torch.zeros(128,requires_grad=True)\n",
        "b3 = torch.zeros(1,requires_grad=True)\n",
        "params = [W1,b1,W2,b2]\n",
        "for param in params:\n",
        "   param.requires_grad_(requires_grad = True)"
      ],
      "metadata": {
        "id": "XUtGXslB4MJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold(fold_num,train_features,train_labels):\n",
        "  \"\"\"\n",
        "  k折交叉验证\n",
        "  \"\"\"\n",
        "  size = len(train_features)\n",
        "  lock_size = size/fold_num\n",
        "  indices = list(range(size))\n",
        "  for i in range(0,fold_num):\n",
        "    temp_indices = torch.tensor(\n",
        "            indices[i*lock_size: (i+1)*lock_size-1])\n",
        "    yield train_features[temp_indices], train_labels[temp_indices]\n"
      ],
      "metadata": {
        "id": "AhSyZhndHN-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **模型训练**\n",
        "通过训练集训练模型调整参数大小，用训练好的网络预测测试集。\n",
        "\n",
        "执行以下的代码将生成一个名为submission.csv的文件。"
      ],
      "metadata": {
        "id": "EZ1NQaMp0hAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_pred(train_features, test_feature, train_labels, test_data,\n",
        "                   num_epochs, lr,batch_size):\n",
        "    # 训练\n",
        "    for epoch in range(num_epochs):\n",
        "      num = 0\n",
        "      for x,y in data_iter(batch_size,train_features,train_labels):\n",
        "        y_hat = net(x,W1,W2,W3,b1,b2,b3)\n",
        "        num+=1\n",
        "        l = CrossEntropyLoss(y_hat,y)\n",
        "        l.sum().backward()\n",
        "        sgd([W1,b1,W2,b2,W3,b3], lr, batch_size)\n",
        "        if num%50 == 0:\n",
        "          print('epoch %d,num %d: loss'%(epoch,num),l)\n",
        "    \n",
        "    # 预测    \n",
        "    predss=net(test_feature,W1,W2,W3,b1,b2,b3).clone()\n",
        "    preds=predss.detach().numpy()\n",
        "    # 将其重新格式化以导出到Kaggle\n",
        "    # preds 是你对所有训练数据的预测结果，形状应该是 (1459, 1) 或类似形状的。（训练数据个数是1459）\n",
        "    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
        "    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n",
        "    submission.to_csv('submission3.csv', index=False)"
      ],
      "metadata": {
        "id": "BWmKFpif0qqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_and_pred(train_features, test_features, train_labels, test_data,\n",
        "               num_epochs, lr, batch_size)"
      ],
      "metadata": {
        "id": "AyDZgkYq1xmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e3dbca-48fd-4171-db27-14028b6fad0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0,num 50: loss tensor(2.5281, grad_fn=<PowBackward0>)\n",
            "epoch 0,num 100: loss tensor(4.6960, grad_fn=<PowBackward0>)\n",
            "epoch 1,num 50: loss tensor(3.1735, grad_fn=<PowBackward0>)\n",
            "epoch 1,num 100: loss tensor(1.0850, grad_fn=<PowBackward0>)\n",
            "epoch 2,num 50: loss tensor(0.3747, grad_fn=<PowBackward0>)\n",
            "epoch 2,num 100: loss tensor(0.5279, grad_fn=<PowBackward0>)\n",
            "epoch 3,num 50: loss tensor(0.3132, grad_fn=<PowBackward0>)\n",
            "epoch 3,num 100: loss tensor(0.3984, grad_fn=<PowBackward0>)\n",
            "epoch 4,num 50: loss tensor(0.4756, grad_fn=<PowBackward0>)\n",
            "epoch 4,num 100: loss tensor(0.6344, grad_fn=<PowBackward0>)\n",
            "epoch 5,num 50: loss tensor(0.5242, grad_fn=<PowBackward0>)\n",
            "epoch 5,num 100: loss tensor(0.3957, grad_fn=<PowBackward0>)\n",
            "epoch 6,num 50: loss tensor(0.3591, grad_fn=<PowBackward0>)\n",
            "epoch 6,num 100: loss tensor(0.3342, grad_fn=<PowBackward0>)\n",
            "epoch 7,num 50: loss tensor(0.5195, grad_fn=<PowBackward0>)\n",
            "epoch 7,num 100: loss tensor(0.5526, grad_fn=<PowBackward0>)\n",
            "epoch 8,num 50: loss tensor(0.5461, grad_fn=<PowBackward0>)\n",
            "epoch 8,num 100: loss tensor(0.3721, grad_fn=<PowBackward0>)\n",
            "epoch 9,num 50: loss tensor(0.2785, grad_fn=<PowBackward0>)\n",
            "epoch 9,num 100: loss tensor(0.3317, grad_fn=<PowBackward0>)\n",
            "epoch 10,num 50: loss tensor(0.3299, grad_fn=<PowBackward0>)\n",
            "epoch 10,num 100: loss tensor(0.2643, grad_fn=<PowBackward0>)\n",
            "epoch 11,num 50: loss tensor(0.2730, grad_fn=<PowBackward0>)\n",
            "epoch 11,num 100: loss tensor(0.3922, grad_fn=<PowBackward0>)\n",
            "epoch 12,num 50: loss tensor(0.3605, grad_fn=<PowBackward0>)\n",
            "epoch 12,num 100: loss tensor(0.2892, grad_fn=<PowBackward0>)\n",
            "epoch 13,num 50: loss tensor(0.2176, grad_fn=<PowBackward0>)\n",
            "epoch 13,num 100: loss tensor(0.2954, grad_fn=<PowBackward0>)\n",
            "epoch 14,num 50: loss tensor(0.2627, grad_fn=<PowBackward0>)\n",
            "epoch 14,num 100: loss tensor(0.4579, grad_fn=<PowBackward0>)\n",
            "epoch 15,num 50: loss tensor(0.4368, grad_fn=<PowBackward0>)\n",
            "epoch 15,num 100: loss tensor(0.3924, grad_fn=<PowBackward0>)\n",
            "epoch 16,num 50: loss tensor(0.1768, grad_fn=<PowBackward0>)\n",
            "epoch 16,num 100: loss tensor(0.3596, grad_fn=<PowBackward0>)\n",
            "epoch 17,num 50: loss tensor(0.3201, grad_fn=<PowBackward0>)\n",
            "epoch 17,num 100: loss tensor(0.3690, grad_fn=<PowBackward0>)\n",
            "epoch 18,num 50: loss tensor(0.3150, grad_fn=<PowBackward0>)\n",
            "epoch 18,num 100: loss tensor(0.3777, grad_fn=<PowBackward0>)\n",
            "epoch 19,num 50: loss tensor(0.2505, grad_fn=<PowBackward0>)\n",
            "epoch 19,num 100: loss tensor(0.3004, grad_fn=<PowBackward0>)\n",
            "epoch 20,num 50: loss tensor(0.2005, grad_fn=<PowBackward0>)\n",
            "epoch 20,num 100: loss tensor(0.3001, grad_fn=<PowBackward0>)\n",
            "epoch 21,num 50: loss tensor(0.3031, grad_fn=<PowBackward0>)\n",
            "epoch 21,num 100: loss tensor(0.2468, grad_fn=<PowBackward0>)\n",
            "epoch 22,num 50: loss tensor(0.3795, grad_fn=<PowBackward0>)\n",
            "epoch 22,num 100: loss tensor(0.2599, grad_fn=<PowBackward0>)\n",
            "epoch 23,num 50: loss tensor(0.3358, grad_fn=<PowBackward0>)\n",
            "epoch 23,num 100: loss tensor(0.2256, grad_fn=<PowBackward0>)\n",
            "epoch 24,num 50: loss tensor(0.1412, grad_fn=<PowBackward0>)\n",
            "epoch 24,num 100: loss tensor(0.2708, grad_fn=<PowBackward0>)\n",
            "epoch 25,num 50: loss tensor(0.3371, grad_fn=<PowBackward0>)\n",
            "epoch 25,num 100: loss tensor(0.2381, grad_fn=<PowBackward0>)\n",
            "epoch 26,num 50: loss tensor(0.1282, grad_fn=<PowBackward0>)\n",
            "epoch 26,num 100: loss tensor(0.2484, grad_fn=<PowBackward0>)\n",
            "epoch 27,num 50: loss tensor(0.2125, grad_fn=<PowBackward0>)\n",
            "epoch 27,num 100: loss tensor(0.1404, grad_fn=<PowBackward0>)\n",
            "epoch 28,num 50: loss tensor(0.2703, grad_fn=<PowBackward0>)\n",
            "epoch 28,num 100: loss tensor(0.3050, grad_fn=<PowBackward0>)\n",
            "epoch 29,num 50: loss tensor(0.1310, grad_fn=<PowBackward0>)\n",
            "epoch 29,num 100: loss tensor(0.2212, grad_fn=<PowBackward0>)\n",
            "epoch 30,num 50: loss tensor(0.1583, grad_fn=<PowBackward0>)\n",
            "epoch 30,num 100: loss tensor(0.2986, grad_fn=<PowBackward0>)\n",
            "epoch 31,num 50: loss tensor(0.2060, grad_fn=<PowBackward0>)\n",
            "epoch 31,num 100: loss tensor(0.2381, grad_fn=<PowBackward0>)\n",
            "epoch 32,num 50: loss tensor(0.1271, grad_fn=<PowBackward0>)\n",
            "epoch 32,num 100: loss tensor(0.2228, grad_fn=<PowBackward0>)\n",
            "epoch 33,num 50: loss tensor(0.2268, grad_fn=<PowBackward0>)\n",
            "epoch 33,num 100: loss tensor(0.1510, grad_fn=<PowBackward0>)\n",
            "epoch 34,num 50: loss tensor(0.1277, grad_fn=<PowBackward0>)\n",
            "epoch 34,num 100: loss tensor(0.2223, grad_fn=<PowBackward0>)\n",
            "epoch 35,num 50: loss tensor(0.1833, grad_fn=<PowBackward0>)\n",
            "epoch 35,num 100: loss tensor(0.1011, grad_fn=<PowBackward0>)\n",
            "epoch 36,num 50: loss tensor(0.0481, grad_fn=<PowBackward0>)\n",
            "epoch 36,num 100: loss tensor(0.2025, grad_fn=<PowBackward0>)\n",
            "epoch 37,num 50: loss tensor(0.0658, grad_fn=<PowBackward0>)\n",
            "epoch 37,num 100: loss tensor(0.1347, grad_fn=<PowBackward0>)\n",
            "epoch 38,num 50: loss tensor(0.1672, grad_fn=<PowBackward0>)\n",
            "epoch 38,num 100: loss tensor(0.1423, grad_fn=<PowBackward0>)\n",
            "epoch 39,num 50: loss tensor(0.1789, grad_fn=<PowBackward0>)\n",
            "epoch 39,num 100: loss tensor(0.0656, grad_fn=<PowBackward0>)\n",
            "epoch 40,num 50: loss tensor(0.0696, grad_fn=<PowBackward0>)\n",
            "epoch 40,num 100: loss tensor(0.1091, grad_fn=<PowBackward0>)\n",
            "epoch 41,num 50: loss tensor(0.1192, grad_fn=<PowBackward0>)\n",
            "epoch 41,num 100: loss tensor(0.1877, grad_fn=<PowBackward0>)\n",
            "epoch 42,num 50: loss tensor(0.0775, grad_fn=<PowBackward0>)\n",
            "epoch 42,num 100: loss tensor(0.1230, grad_fn=<PowBackward0>)\n",
            "epoch 43,num 50: loss tensor(0.0935, grad_fn=<PowBackward0>)\n",
            "epoch 43,num 100: loss tensor(0.1285, grad_fn=<PowBackward0>)\n",
            "epoch 44,num 50: loss tensor(0.1471, grad_fn=<PowBackward0>)\n",
            "epoch 44,num 100: loss tensor(0.0641, grad_fn=<PowBackward0>)\n",
            "epoch 45,num 50: loss tensor(0.0985, grad_fn=<PowBackward0>)\n",
            "epoch 45,num 100: loss tensor(0.1753, grad_fn=<PowBackward0>)\n",
            "epoch 46,num 50: loss tensor(0.1386, grad_fn=<PowBackward0>)\n",
            "epoch 46,num 100: loss tensor(0.0875, grad_fn=<PowBackward0>)\n",
            "epoch 47,num 50: loss tensor(0.0934, grad_fn=<PowBackward0>)\n",
            "epoch 47,num 100: loss tensor(0.1071, grad_fn=<PowBackward0>)\n",
            "epoch 48,num 50: loss tensor(0.0788, grad_fn=<PowBackward0>)\n",
            "epoch 48,num 100: loss tensor(0.0785, grad_fn=<PowBackward0>)\n",
            "epoch 49,num 50: loss tensor(0.0670, grad_fn=<PowBackward0>)\n",
            "epoch 49,num 100: loss tensor(0.1384, grad_fn=<PowBackward0>)\n",
            "epoch 50,num 50: loss tensor(0.0850, grad_fn=<PowBackward0>)\n",
            "epoch 50,num 100: loss tensor(0.1448, grad_fn=<PowBackward0>)\n",
            "epoch 51,num 50: loss tensor(0.1148, grad_fn=<PowBackward0>)\n",
            "epoch 51,num 100: loss tensor(0.0962, grad_fn=<PowBackward0>)\n",
            "epoch 52,num 50: loss tensor(0.1288, grad_fn=<PowBackward0>)\n",
            "epoch 52,num 100: loss tensor(0.1394, grad_fn=<PowBackward0>)\n",
            "epoch 53,num 50: loss tensor(0.0782, grad_fn=<PowBackward0>)\n",
            "epoch 53,num 100: loss tensor(0.0909, grad_fn=<PowBackward0>)\n",
            "epoch 54,num 50: loss tensor(0.0582, grad_fn=<PowBackward0>)\n",
            "epoch 54,num 100: loss tensor(0.1059, grad_fn=<PowBackward0>)\n",
            "epoch 55,num 50: loss tensor(0.0624, grad_fn=<PowBackward0>)\n",
            "epoch 55,num 100: loss tensor(0.0596, grad_fn=<PowBackward0>)\n",
            "epoch 56,num 50: loss tensor(0.0430, grad_fn=<PowBackward0>)\n",
            "epoch 56,num 100: loss tensor(0.0730, grad_fn=<PowBackward0>)\n",
            "epoch 57,num 50: loss tensor(0.1125, grad_fn=<PowBackward0>)\n",
            "epoch 57,num 100: loss tensor(0.1235, grad_fn=<PowBackward0>)\n",
            "epoch 58,num 50: loss tensor(0.0737, grad_fn=<PowBackward0>)\n",
            "epoch 58,num 100: loss tensor(0.0774, grad_fn=<PowBackward0>)\n",
            "epoch 59,num 50: loss tensor(0.0950, grad_fn=<PowBackward0>)\n",
            "epoch 59,num 100: loss tensor(0.1118, grad_fn=<PowBackward0>)\n",
            "epoch 60,num 50: loss tensor(0.0600, grad_fn=<PowBackward0>)\n",
            "epoch 60,num 100: loss tensor(0.0802, grad_fn=<PowBackward0>)\n",
            "epoch 61,num 50: loss tensor(0.1020, grad_fn=<PowBackward0>)\n",
            "epoch 61,num 100: loss tensor(0.1052, grad_fn=<PowBackward0>)\n",
            "epoch 62,num 50: loss tensor(0.0875, grad_fn=<PowBackward0>)\n",
            "epoch 62,num 100: loss tensor(0.1415, grad_fn=<PowBackward0>)\n",
            "epoch 63,num 50: loss tensor(0.0444, grad_fn=<PowBackward0>)\n",
            "epoch 63,num 100: loss tensor(0.0739, grad_fn=<PowBackward0>)\n",
            "epoch 64,num 50: loss tensor(0.0830, grad_fn=<PowBackward0>)\n",
            "epoch 64,num 100: loss tensor(0.0963, grad_fn=<PowBackward0>)\n",
            "epoch 65,num 50: loss tensor(0.0812, grad_fn=<PowBackward0>)\n",
            "epoch 65,num 100: loss tensor(0.0893, grad_fn=<PowBackward0>)\n",
            "epoch 66,num 50: loss tensor(0.0661, grad_fn=<PowBackward0>)\n",
            "epoch 66,num 100: loss tensor(0.0592, grad_fn=<PowBackward0>)\n",
            "epoch 67,num 50: loss tensor(0.1020, grad_fn=<PowBackward0>)\n",
            "epoch 67,num 100: loss tensor(0.1317, grad_fn=<PowBackward0>)\n",
            "epoch 68,num 50: loss tensor(0.1846, grad_fn=<PowBackward0>)\n",
            "epoch 68,num 100: loss tensor(0.0494, grad_fn=<PowBackward0>)\n",
            "epoch 69,num 50: loss tensor(0.0595, grad_fn=<PowBackward0>)\n",
            "epoch 69,num 100: loss tensor(0.0909, grad_fn=<PowBackward0>)\n",
            "epoch 70,num 50: loss tensor(0.0762, grad_fn=<PowBackward0>)\n",
            "epoch 70,num 100: loss tensor(0.1253, grad_fn=<PowBackward0>)\n",
            "epoch 71,num 50: loss tensor(0.1204, grad_fn=<PowBackward0>)\n",
            "epoch 71,num 100: loss tensor(0.0644, grad_fn=<PowBackward0>)\n",
            "epoch 72,num 50: loss tensor(0.0847, grad_fn=<PowBackward0>)\n",
            "epoch 72,num 100: loss tensor(0.0696, grad_fn=<PowBackward0>)\n",
            "epoch 73,num 50: loss tensor(0.0636, grad_fn=<PowBackward0>)\n",
            "epoch 73,num 100: loss tensor(0.1141, grad_fn=<PowBackward0>)\n",
            "epoch 74,num 50: loss tensor(0.0423, grad_fn=<PowBackward0>)\n",
            "epoch 74,num 100: loss tensor(0.2018, grad_fn=<PowBackward0>)\n",
            "epoch 75,num 50: loss tensor(0.1394, grad_fn=<PowBackward0>)\n",
            "epoch 75,num 100: loss tensor(0.0867, grad_fn=<PowBackward0>)\n",
            "epoch 76,num 50: loss tensor(0.0740, grad_fn=<PowBackward0>)\n",
            "epoch 76,num 100: loss tensor(0.0523, grad_fn=<PowBackward0>)\n",
            "epoch 77,num 50: loss tensor(0.1298, grad_fn=<PowBackward0>)\n",
            "epoch 77,num 100: loss tensor(0.0329, grad_fn=<PowBackward0>)\n",
            "epoch 78,num 50: loss tensor(0.0425, grad_fn=<PowBackward0>)\n",
            "epoch 78,num 100: loss tensor(0.0940, grad_fn=<PowBackward0>)\n",
            "epoch 79,num 50: loss tensor(0.0574, grad_fn=<PowBackward0>)\n",
            "epoch 79,num 100: loss tensor(0.0451, grad_fn=<PowBackward0>)\n",
            "epoch 80,num 50: loss tensor(0.0661, grad_fn=<PowBackward0>)\n",
            "epoch 80,num 100: loss tensor(0.0747, grad_fn=<PowBackward0>)\n",
            "epoch 81,num 50: loss tensor(0.0535, grad_fn=<PowBackward0>)\n",
            "epoch 81,num 100: loss tensor(0.0644, grad_fn=<PowBackward0>)\n",
            "epoch 82,num 50: loss tensor(0.0712, grad_fn=<PowBackward0>)\n",
            "epoch 82,num 100: loss tensor(0.1283, grad_fn=<PowBackward0>)\n",
            "epoch 83,num 50: loss tensor(0.0553, grad_fn=<PowBackward0>)\n",
            "epoch 83,num 100: loss tensor(0.0381, grad_fn=<PowBackward0>)\n",
            "epoch 84,num 50: loss tensor(0.0574, grad_fn=<PowBackward0>)\n",
            "epoch 84,num 100: loss tensor(0.0360, grad_fn=<PowBackward0>)\n",
            "epoch 85,num 50: loss tensor(0.0317, grad_fn=<PowBackward0>)\n",
            "epoch 85,num 100: loss tensor(0.0594, grad_fn=<PowBackward0>)\n",
            "epoch 86,num 50: loss tensor(0.0509, grad_fn=<PowBackward0>)\n",
            "epoch 86,num 100: loss tensor(0.0538, grad_fn=<PowBackward0>)\n",
            "epoch 87,num 50: loss tensor(0.0404, grad_fn=<PowBackward0>)\n",
            "epoch 87,num 100: loss tensor(0.0468, grad_fn=<PowBackward0>)\n",
            "epoch 88,num 50: loss tensor(0.0746, grad_fn=<PowBackward0>)\n",
            "epoch 88,num 100: loss tensor(0.0617, grad_fn=<PowBackward0>)\n",
            "epoch 89,num 50: loss tensor(0.0294, grad_fn=<PowBackward0>)\n",
            "epoch 89,num 100: loss tensor(0.0658, grad_fn=<PowBackward0>)\n",
            "epoch 90,num 50: loss tensor(0.0850, grad_fn=<PowBackward0>)\n",
            "epoch 90,num 100: loss tensor(0.0345, grad_fn=<PowBackward0>)\n",
            "epoch 91,num 50: loss tensor(0.0386, grad_fn=<PowBackward0>)\n",
            "epoch 91,num 100: loss tensor(0.0305, grad_fn=<PowBackward0>)\n",
            "epoch 92,num 50: loss tensor(0.0585, grad_fn=<PowBackward0>)\n",
            "epoch 92,num 100: loss tensor(0.0529, grad_fn=<PowBackward0>)\n",
            "epoch 93,num 50: loss tensor(0.0692, grad_fn=<PowBackward0>)\n",
            "epoch 93,num 100: loss tensor(0.0980, grad_fn=<PowBackward0>)\n",
            "epoch 94,num 50: loss tensor(0.0630, grad_fn=<PowBackward0>)\n",
            "epoch 94,num 100: loss tensor(0.0511, grad_fn=<PowBackward0>)\n",
            "epoch 95,num 50: loss tensor(0.0261, grad_fn=<PowBackward0>)\n",
            "epoch 95,num 100: loss tensor(0.0363, grad_fn=<PowBackward0>)\n",
            "epoch 96,num 50: loss tensor(0.0506, grad_fn=<PowBackward0>)\n",
            "epoch 96,num 100: loss tensor(0.0525, grad_fn=<PowBackward0>)\n",
            "epoch 97,num 50: loss tensor(0.0354, grad_fn=<PowBackward0>)\n",
            "epoch 97,num 100: loss tensor(0.0832, grad_fn=<PowBackward0>)\n",
            "epoch 98,num 50: loss tensor(0.0402, grad_fn=<PowBackward0>)\n",
            "epoch 98,num 100: loss tensor(0.0543, grad_fn=<PowBackward0>)\n",
            "epoch 99,num 50: loss tensor(0.0745, grad_fn=<PowBackward0>)\n",
            "epoch 99,num 100: loss tensor(0.0479, grad_fn=<PowBackward0>)\n",
            "epoch 100,num 50: loss tensor(0.0511, grad_fn=<PowBackward0>)\n",
            "epoch 100,num 100: loss tensor(0.0523, grad_fn=<PowBackward0>)\n",
            "epoch 101,num 50: loss tensor(0.0594, grad_fn=<PowBackward0>)\n",
            "epoch 101,num 100: loss tensor(0.0656, grad_fn=<PowBackward0>)\n",
            "epoch 102,num 50: loss tensor(0.0631, grad_fn=<PowBackward0>)\n",
            "epoch 102,num 100: loss tensor(0.0249, grad_fn=<PowBackward0>)\n",
            "epoch 103,num 50: loss tensor(0.0465, grad_fn=<PowBackward0>)\n",
            "epoch 103,num 100: loss tensor(0.0344, grad_fn=<PowBackward0>)\n",
            "epoch 104,num 50: loss tensor(0.0487, grad_fn=<PowBackward0>)\n",
            "epoch 104,num 100: loss tensor(0.0490, grad_fn=<PowBackward0>)\n",
            "epoch 105,num 50: loss tensor(0.0359, grad_fn=<PowBackward0>)\n",
            "epoch 105,num 100: loss tensor(0.0394, grad_fn=<PowBackward0>)\n",
            "epoch 106,num 50: loss tensor(0.0296, grad_fn=<PowBackward0>)\n",
            "epoch 106,num 100: loss tensor(0.0308, grad_fn=<PowBackward0>)\n",
            "epoch 107,num 50: loss tensor(0.0614, grad_fn=<PowBackward0>)\n",
            "epoch 107,num 100: loss tensor(0.0504, grad_fn=<PowBackward0>)\n",
            "epoch 108,num 50: loss tensor(0.0649, grad_fn=<PowBackward0>)\n",
            "epoch 108,num 100: loss tensor(0.0571, grad_fn=<PowBackward0>)\n",
            "epoch 109,num 50: loss tensor(0.0541, grad_fn=<PowBackward0>)\n",
            "epoch 109,num 100: loss tensor(0.0461, grad_fn=<PowBackward0>)\n",
            "epoch 110,num 50: loss tensor(0.0484, grad_fn=<PowBackward0>)\n",
            "epoch 110,num 100: loss tensor(0.0837, grad_fn=<PowBackward0>)\n",
            "epoch 111,num 50: loss tensor(0.0336, grad_fn=<PowBackward0>)\n",
            "epoch 111,num 100: loss tensor(0.0421, grad_fn=<PowBackward0>)\n",
            "epoch 112,num 50: loss tensor(0.0825, grad_fn=<PowBackward0>)\n",
            "epoch 112,num 100: loss tensor(0.0432, grad_fn=<PowBackward0>)\n",
            "epoch 113,num 50: loss tensor(0.0460, grad_fn=<PowBackward0>)\n",
            "epoch 113,num 100: loss tensor(0.0393, grad_fn=<PowBackward0>)\n",
            "epoch 114,num 50: loss tensor(0.0486, grad_fn=<PowBackward0>)\n",
            "epoch 114,num 100: loss tensor(0.0459, grad_fn=<PowBackward0>)\n",
            "epoch 115,num 50: loss tensor(0.0333, grad_fn=<PowBackward0>)\n",
            "epoch 115,num 100: loss tensor(0.0589, grad_fn=<PowBackward0>)\n",
            "epoch 116,num 50: loss tensor(0.0298, grad_fn=<PowBackward0>)\n",
            "epoch 116,num 100: loss tensor(0.0486, grad_fn=<PowBackward0>)\n",
            "epoch 117,num 50: loss tensor(0.0487, grad_fn=<PowBackward0>)\n",
            "epoch 117,num 100: loss tensor(0.0441, grad_fn=<PowBackward0>)\n",
            "epoch 118,num 50: loss tensor(0.0681, grad_fn=<PowBackward0>)\n",
            "epoch 118,num 100: loss tensor(0.0685, grad_fn=<PowBackward0>)\n",
            "epoch 119,num 50: loss tensor(0.0349, grad_fn=<PowBackward0>)\n",
            "epoch 119,num 100: loss tensor(0.0381, grad_fn=<PowBackward0>)\n",
            "epoch 120,num 50: loss tensor(0.0575, grad_fn=<PowBackward0>)\n",
            "epoch 120,num 100: loss tensor(0.0372, grad_fn=<PowBackward0>)\n",
            "epoch 121,num 50: loss tensor(0.0376, grad_fn=<PowBackward0>)\n",
            "epoch 121,num 100: loss tensor(0.0320, grad_fn=<PowBackward0>)\n",
            "epoch 122,num 50: loss tensor(0.0265, grad_fn=<PowBackward0>)\n",
            "epoch 122,num 100: loss tensor(0.0154, grad_fn=<PowBackward0>)\n",
            "epoch 123,num 50: loss tensor(0.0212, grad_fn=<PowBackward0>)\n",
            "epoch 123,num 100: loss tensor(0.0418, grad_fn=<PowBackward0>)\n",
            "epoch 124,num 50: loss tensor(0.0905, grad_fn=<PowBackward0>)\n",
            "epoch 124,num 100: loss tensor(0.0465, grad_fn=<PowBackward0>)\n",
            "epoch 125,num 50: loss tensor(0.0392, grad_fn=<PowBackward0>)\n",
            "epoch 125,num 100: loss tensor(0.0492, grad_fn=<PowBackward0>)\n",
            "epoch 126,num 50: loss tensor(0.0450, grad_fn=<PowBackward0>)\n",
            "epoch 126,num 100: loss tensor(0.0372, grad_fn=<PowBackward0>)\n",
            "epoch 127,num 50: loss tensor(0.0671, grad_fn=<PowBackward0>)\n",
            "epoch 127,num 100: loss tensor(0.0481, grad_fn=<PowBackward0>)\n",
            "epoch 128,num 50: loss tensor(0.0292, grad_fn=<PowBackward0>)\n",
            "epoch 128,num 100: loss tensor(0.0216, grad_fn=<PowBackward0>)\n",
            "epoch 129,num 50: loss tensor(0.0447, grad_fn=<PowBackward0>)\n",
            "epoch 129,num 100: loss tensor(0.0308, grad_fn=<PowBackward0>)\n",
            "epoch 130,num 50: loss tensor(0.0552, grad_fn=<PowBackward0>)\n",
            "epoch 130,num 100: loss tensor(0.0714, grad_fn=<PowBackward0>)\n",
            "epoch 131,num 50: loss tensor(0.0319, grad_fn=<PowBackward0>)\n",
            "epoch 131,num 100: loss tensor(0.0530, grad_fn=<PowBackward0>)\n",
            "epoch 132,num 50: loss tensor(0.0362, grad_fn=<PowBackward0>)\n",
            "epoch 132,num 100: loss tensor(0.0304, grad_fn=<PowBackward0>)\n",
            "epoch 133,num 50: loss tensor(0.0379, grad_fn=<PowBackward0>)\n",
            "epoch 133,num 100: loss tensor(0.0152, grad_fn=<PowBackward0>)\n",
            "epoch 134,num 50: loss tensor(0.0511, grad_fn=<PowBackward0>)\n",
            "epoch 134,num 100: loss tensor(0.0716, grad_fn=<PowBackward0>)\n",
            "epoch 135,num 50: loss tensor(0.0520, grad_fn=<PowBackward0>)\n",
            "epoch 135,num 100: loss tensor(0.0609, grad_fn=<PowBackward0>)\n",
            "epoch 136,num 50: loss tensor(0.0426, grad_fn=<PowBackward0>)\n",
            "epoch 136,num 100: loss tensor(0.0339, grad_fn=<PowBackward0>)\n",
            "epoch 137,num 50: loss tensor(0.0546, grad_fn=<PowBackward0>)\n",
            "epoch 137,num 100: loss tensor(0.0408, grad_fn=<PowBackward0>)\n",
            "epoch 138,num 50: loss tensor(0.0289, grad_fn=<PowBackward0>)\n",
            "epoch 138,num 100: loss tensor(0.0366, grad_fn=<PowBackward0>)\n",
            "epoch 139,num 50: loss tensor(0.0358, grad_fn=<PowBackward0>)\n",
            "epoch 139,num 100: loss tensor(0.0259, grad_fn=<PowBackward0>)\n",
            "epoch 140,num 50: loss tensor(0.0262, grad_fn=<PowBackward0>)\n",
            "epoch 140,num 100: loss tensor(0.0338, grad_fn=<PowBackward0>)\n",
            "epoch 141,num 50: loss tensor(0.0271, grad_fn=<PowBackward0>)\n",
            "epoch 141,num 100: loss tensor(0.0408, grad_fn=<PowBackward0>)\n",
            "epoch 142,num 50: loss tensor(0.0783, grad_fn=<PowBackward0>)\n",
            "epoch 142,num 100: loss tensor(0.0559, grad_fn=<PowBackward0>)\n",
            "epoch 143,num 50: loss tensor(0.0317, grad_fn=<PowBackward0>)\n",
            "epoch 143,num 100: loss tensor(0.0577, grad_fn=<PowBackward0>)\n",
            "epoch 144,num 50: loss tensor(0.0723, grad_fn=<PowBackward0>)\n",
            "epoch 144,num 100: loss tensor(0.0483, grad_fn=<PowBackward0>)\n",
            "epoch 145,num 50: loss tensor(0.0362, grad_fn=<PowBackward0>)\n",
            "epoch 145,num 100: loss tensor(0.0678, grad_fn=<PowBackward0>)\n",
            "epoch 146,num 50: loss tensor(0.0499, grad_fn=<PowBackward0>)\n",
            "epoch 146,num 100: loss tensor(0.0361, grad_fn=<PowBackward0>)\n",
            "epoch 147,num 50: loss tensor(0.0203, grad_fn=<PowBackward0>)\n",
            "epoch 147,num 100: loss tensor(0.0507, grad_fn=<PowBackward0>)\n",
            "epoch 148,num 50: loss tensor(0.0925, grad_fn=<PowBackward0>)\n",
            "epoch 148,num 100: loss tensor(0.0351, grad_fn=<PowBackward0>)\n",
            "epoch 149,num 50: loss tensor(0.0273, grad_fn=<PowBackward0>)\n",
            "epoch 149,num 100: loss tensor(0.0500, grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    }
  ]
}